# -*- coding: utf-8 -*-
"""detect malicious URLs

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15nL3OwhDPLo-V6ObTUfxII0WJl3nRbw0
"""

import numpy as np
import os
import time
import datetime
import csv
import sys
import logging
import multiprocessing
import json
import pandas as pd
import random
import pickle
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
from sklearn.model_selection import train_test_split
import tempfile

'''
python word2vec_helpers.py input_file output_model_file output_vector_file
'''

# import modules & set up logging

 
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence

def output_vocab(vocab):
    for k, v in vocab.items():
        print(k)

def embedding_sentences(sentences, embedding_size = 128, window = 5, min_count = 5, file_to_load = None, file_to_save = None):
    '''
    embeding_size Word embedding dimension
    window : Context window
    min_count : Words with frequency less than min_count will be deleted
    '''
    if file_to_load is not None:
        w2vModel = Word2Vec.load(file_to_load)
    else:
        w2vModel = Word2Vec(sentences, size = embedding_size, window = window, min_count = min_count, workers = multiprocessing.cpu_count())
        if file_to_save is not None:
            w2vModel.save(file_to_save)

    all_vectors = []
    embeddingDim = w2vModel.vector_size
    # Embedding dimension
    embeddingUnknown = [0 for i in range(embeddingDim)]
    for sentence in sentences:
        this_vector = []
        for word in sentence:
            if word in w2vModel.wv.vocab:
                this_vector.append(w2vModel[word])
            else:
                this_vector.append(embeddingUnknown)
        all_vectors.append(this_vector)
    return all_vectors


def generate_word2vec_files(input_file, output_model_file, output_vector_file, size = 128, window = 5, min_count = 5):
    start_time = time.time()

    # trim unneeded model memory = use(much) less RAM
    # model.init_sims(replace=True)
    model = Word2Vec(LineSentence(input_file), size = size, window = window, min_count = min_count, workers = multiprocessing.cpu_count())
    model.save(output_model_file)
    model.wv.save_word2vec_format(output_vector_file, binary=False)

    end_time = time.time()
    print("used time : %d s" % (end_time - start_time))

def run_main():
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)
 
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    logger.info("running %s" % ' '.join(sys.argv))
 
    # check and process input arguments
    if len(sys.argv) < 4:
        print (globals()['__doc__'] % locals())
        sys.exit(1)
    input_file, output_model_file, output_vector_file = sys.argv[1:4]

    generate_word2vec_files(input_file, output_model_file, output_vector_file) 

def test():
    vectors = embedding_sentences([['first', 'sentence'], ['second', 'sentence']], embedding_size = 4, min_count = 1)
    print(vectors)

def to_categorical(y, nb_classes=None):
    """Converts a class vector (integers) to binary class matrix.

    E.g. for use with categorical_crossentropy.

    # Arguments
        y: class vector to be converted into a matrix
            (integers from 0 to nb_classes).
        nb_classes: total number of classes.

    # Returns
        A binary matrix representation of the input.
    """
    y = np.array(y, dtype='int').ravel()
    if not nb_classes:
        nb_classes = np.max(y) + 1
    n = y.shape[0]
    categorical = np.zeros((n, nb_classes))
    categorical[np.arange(n), y] = 1
    return categorical

def getTokens(input):
    # tokensBySlash = str(input.encode('utf-8')).split('/')
    # print(tokensBySlash)
    # tokensBySlash = str(input).split('/')
        allTokens = []
    # for i in range(0, len(tokensBySlash)):
        token = str(input)
        # token = tokensBySlash[i]
        tokens = []
        token = token.replace('.', '/')
        token = token.replace('=', '/')
        token = token.replace('&', '/')
        token = token.replace('?', '/')
        token = token.replace('-', '/')
        token = token.replace('@', '/')
        token = token.replace(':', '/')
        tokens = token.split('/')
        allTokens = allTokens + tokens
        # allTokens = list(set(allTokens))  # remove redundant tokens
        return allTokens


def load_data_and_labels(path):
    # allurls = '/Users/zcw/Documents/python/DetectMaliciousURL/data/data.csv'	#path to our all urls file
    allurlscsv = pd.read_csv(path, ',', error_bad_lines=False)  # reading file
    allurlsdata = pd.DataFrame(allurlscsv)  # converting to a dataframe
    allurlsdata = np.array(allurlsdata)  # converting it into an array
    random.shuffle(allurlsdata)  # shuffling
    y = [d[1] for d in allurlsdata]  # all labels
    x = [d[0] for d in allurlsdata]  # all urls corresponding to a label (either good or bad)
    for i in range(0,len(y)):
        if y[i] =='bad':
            y[i]=0
        else:
            y[i]=1
    label = to_categorical(y, 2)
    return (x, label)

def padding_sentences(input_sentences, padding_token, padding_sentence_length=None):
    sentences = [getTokens(sentence) for sentence in input_sentences]
    max_sentence_length = padding_sentence_length if padding_sentence_length is not None else max(
            [len(sentence) for sentence in sentences])
    i=0
    all_vector=[]
    for sentence in sentences:
        if len(sentence) > max_sentence_length:
            sentence = sentence[:max_sentence_length]
        else:
            sentence.extend([padding_token] * (max_sentence_length - len(sentence)))
        all_vector.append(sentence)
    return (all_vector, max_sentence_length)


def saveDict(input_dict, output_file):
    with open(output_file, 'wb') as f:
        pickle.dump(input_dict, f)


def loadDict(dict_file):
    output_dict = None
    with open(dict_file, 'rb') as f:
        output_dict = pickle.load(f)
    return output_dict


def batch_iter(data, batch_size, num_epochs, shuffle=True):
    '''
    Generate a batch iterator for a dataset
    '''
    data = np.array(data)
    data_size = len(data)
    num_batches_per_epoch = int((data_size - 1) / batch_size) + 1
    for epoch in range(num_epochs):
        if shuffle:
            # Shuffle the data at each epoch
            shuffle_indices = np.random.permutation(np.arange(data_size))
            shuffled_data = data[shuffle_indices]
        else:
            shuffled_data = data
        for batch_num in range(num_batches_per_epoch):
            start_idx = batch_num * batch_size
            end_idx = min((batch_num + 1) * batch_size, data_size)
            yield shuffled_data[start_idx: end_idx]


if __name__ == "__main__":
    test = "rapiseebrains.com/?a=401336&c=cpc&s=050217"
    ans = getTokens(test)
    print (ans, len(ans))

class URLCNN(object):
    '''
    A CNN for URL classification
    Uses and embedding layer, followed by a convolutional, max-pooling and softmax layer.
    '''
    def __init__(
            self, sequence_length, num_classes,
            embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):
        # Placeholders for input, output, dropout
        self.input_x = tf.compat.v1.placeholder(tf.float32, [None, sequence_length, embedding_size], name="input_x")
        self.input_y = tf.compat.v1.placeholder(tf.float32, [None, num_classes], name="input_y")
        self.dropout_keep_prob = tf.compat.v1.placeholder(tf.float32, name="dropout_keep_prob")

        # Keeping track of l2 regularization loss (optional)
        l2_loss = tf.constant(0.0)

        # Embedding layer
        # self.embedded_chars = [None(batch_size), sequence_size, embedding_size]
        # self.embedded_chars = [None(batch_size), sequence_size, embedding_size, 1(num_channels)]
        self.embedded_chars = self.input_x
        self.embedded_chars_expended = tf.expand_dims(self.embedded_chars, -1)

        # Create a convolution + maxpool layer for each filter size
        pooled_outputs = []
        for i, filter_size in enumerate(filter_sizes):
            with tf.compat.v1.name_scope("conv-maxpool-%s" % filter_size):
                # Convolution layer
                filter_shape = [filter_size, embedding_size, 1, num_filters]
                W = tf.Variable(tf.random.truncated_normal(filter_shape, stddev=0.1), name="W")
                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name="b")
                conv = tf.nn.conv2d(
                        input=self.embedded_chars_expended,
                        filters=W,
                        strides=[1, 1, 1, 1],
                        padding="VALID",
                        name="conv")
                # Apply nonlinearity
                h = tf.nn.relu(tf.nn.bias_add(conv, b), name="relu")
                # Maxpooling over the outputs
                pooled = tf.nn.max_pool2d(
                        input=h,
                        ksize=[1, sequence_length - filter_size + 1, 1, 1],
                        strides=[1, 1, 1, 1],
                        padding="VALID",
                        name="pool")
                pooled_outputs.append(pooled)

        # Combine all the pooled features
        num_filters_total = num_filters * len(filter_sizes)
        self.h_pool = tf.concat(pooled_outputs, 3)
        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])

        # Add dropout
        # with tf.name_scope("dropout"):
        #     self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)
        epsilon = 1e-3
        with tf.compat.v1.name_scope("BATCH-NORM"):
            batch_mean,batch_var = tf.nn.moments(x=self.h_pool_flat,axes=[0])
            scale = tf.Variable(tf.ones([384]))
            beta = tf.Variable(tf.zeros([384]))
            self.BN = tf.nn.batch_normalization(self.h_pool_flat,batch_mean,batch_var,beta,scale,epsilon)

        # Add 2-layer-MLP
        h1_units=128
        h2_units=64
        with tf.compat.v1.name_scope("FC-Layer-1"):
            W = tf.Variable(tf.random.truncated_normal(shape=[384,h1_units], stddev=0.1), name="W")
            b = tf.Variable(tf.constant(0.1, shape=[h1_units]), name="b")
            self.hidden_1 = tf.nn.relu(tf.compat.v1.nn.xw_plus_b(self.BN,W,b,name="fc1"))
        with tf.compat.v1.name_scope("FC-Layer-2"):
            W = tf.Variable(tf.random.truncated_normal(shape=[h1_units,h2_units], stddev=0.1), name="W")
            b = tf.Variable(tf.constant(0.1, shape=[h2_units]), name="b")
            self.hidden_2 = tf.nn.relu(tf.compat.v1.nn.xw_plus_b(self.hidden_1,W,b,name="hidden"))

        # Final (unnomalized) scores and predictions
        with tf.compat.v1.name_scope("output"):
            W = tf.compat.v1.get_variable(
                    "W",
                    # shape=[num_filters_total, num_classes],
                    shape=[h2_units,num_classes],
                    initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode="fan_avg", distribution="uniform"))
            b = tf.Variable(tf.constant(0.1, shape=[num_classes], name="b"))
            l2_loss += tf.nn.l2_loss(W)
            l2_loss += tf.nn.l2_loss(b)
            self.scores = tf.compat.v1.nn.xw_plus_b(self.hidden_2, W, b, name="scores")
            self.predictions = tf.argmax(input=self.scores, axis=1, name="predictions")

        # Calculate Mean cross-entropy loss
        with tf.compat.v1.name_scope("loss"):
            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=tf.stop_gradient(self.input_y))
            self.loss = tf.reduce_mean(input_tensor=losses) + l2_reg_lambda * l2_loss

        # Accuracy
        with tf.compat.v1.name_scope("accuracy"):
            correct_predictions = tf.equal(self.predictions, tf.argmax(input=self.input_y, axis=1))
            self.accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_predictions, "float"), name="accuracy")

# Parameters
# =======================================================

# Data loading parameters
data_file = "data.csv"
num_labels = 2
#
# # Model hyperparameters
embedding_dim = 64
filter_sizes = 3
num_filters = 384 
dropout_keep_prob = 0.5
l2_reg_lambda = 0.0
sequence_length = 100
#
# # Training paramters
batch_size = 32
num_epochs = 200
evaluate_every = 100
checkpoint_every = 100
num_checkpoints = 5
#
# # Misc parameters
allow_soft_placement = True
log_device_placement = False
# # Distribution
replicas = False,
is_sync = False, 
ps_hosts = "192.168.0.107:2222"
worker_hosts = "10.211.55.14:2222, 10.211.55.13:2222"
job_name = None
task_index = 0
log_dir = "summary"
# Parse parameters from commands

 # out_dir = os.path.abspath(os.path.join(os.path.curdir, "runs", "replicas"))
out_dir = os.path.abspath(os.path.join(os.path.curdir,"runs","outputs"))
print("Writing to {}\n".format(out_dir))
if not os.path.exists(out_dir):
   os.makedirs(out_dir)
checkpoint_dir = os.path.abspath(os.path.join(out_dir, "checkpoints"))
checkpoint_prefix = os.path.join(checkpoint_dir, "model")
if not os.path.exists(checkpoint_dir):
       os.makedirs(checkpoint_dir)
summary_dir= os.path.abspath(os.path.join(out_dir, "summary"))
if not os.path.exists(summary_dir):
       os.makedirs(summary_dir)
train_summary_dir = os.path.join(summary_dir,"train")
dev_summary_dir= os.path.join(summary_dir,"dev")
if not os.path.exists(train_summary_dir):
       os.makedirs(train_summary_dir)
if not os.path.exists(dev_summary_dir):
       os.makedirs(dev_summary_dir)

def data_preprocess():
    # Data preprocess
    # =======================================================
    # Load data
    print("Loading data...")
    if not os.path.exists(os.path.join(out_dir,"data_x.npy")):
          x, y = load_data_and_labels(data_file)
          # Get embedding vector
          x =x[:1000]
          y =y[:1000]
          sentences, max_document_length = padding_sentences(x, '<PADDING>',padding_sentence_length=sequence_length)
          print(len(sentences[0]))
          if not os.path.exists(os.path.join(out_dir,"trained_word2vec.model")):
              x= np.array(embedding_sentences(sentences, embedding_size = embedding_dim, file_to_save = os.path.join(out_dir, 'trained_word2vec.model')))
          else:
              print('w2v model found...')
              x = np.array(embedding_sentences(sentences, embedding_size = embedding_dim, file_to_save = os.path.join(out_dir, 'trained_word2vec.model'),file_to_load=os.path.join(out_dir, 'trained_word2vec.model')))
          y = np.array(y)
          # np.save(os.path.join(out_dir,"data_x.npy"),x)
          # np.save(os.path.join(out_dir,"data_y.npy"),y)
          del sentences
    else:
          print('data found...')
          x= np.load(os.path.join(out_dir,"data_x.npy"))
          y= np.load(os.path.join(out_dir,"data_y.npy"))
    print("x.shape = {}".format(x.shape))
    print("y.shape = {}".format(y.shape))

    # Save params
    if not os.path.exists(os.path.join(out_dir,"training_params.pickle")):
        training_params_file = os.path.join(out_dir, 'training_params.pickle')
        params = {'num_labels' : num_labels, 'max_document_length' : max_document_length}
        saveDict(params, training_params_file)

    # Shuffle data randomly
    # np.random.seed(10)
    # shuffle_indices = np.random.permutation(np.arange(len(y)))
    # x_shuffled = x[shuffle_indices]
    # y_shuffled = y[shuffle_indices]
    # del x,y

    # x_train, x_test, y_train, y_test = train_test_split(x_shuffled, y_shuffled, test_size=0.2, random_state=42)  # split into training and testing set 80/20 ratio
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)  # split into training and testing set 80/20 ratio
    del x,y
    return x_train, x_test, y_train, y_test

if replicas==True:
    if job_name is None or job_name =="":
        raise ValueError("Must specify an explicit 'job_name")
    if task_index is None or FLAGS.task_index =="":
        raise ValueError("Must specify an explicit task_index")
    print("job name = %s" % job_name)
    print("task index = %d" % task_index)
    ps_spec = ps_hosts.split(",")
    worker_spec = worker_hosts.split(",")
    num_workers = len(worker_spec)
    cluster = tf.train.ClusterSpec({"ps": ps_spec,"worker":worker_spec})
    server = tf.train.Server(
                cluster, job_name=job_name,task_index=task_index
    )
    if job_name =="ps":
        server.join()

    elif job_name == "worker":
      x_train, x_test, y_train, y_test =data_preprocess()
      with tf.Graph().as_default():
        # Assigns ops to the local worker by default.
        with tf.device(tf.train.replica_device_setter(
            worker_device="/job:worker/task:%d" % task_index,
            cluster=cluster)):
                 global_step = tf.Variable(0, name="global_step", trainable=False)
                 cnn = URLCNN(
                       sequence_length = sequence_length,
                       num_classes = num_labels,
                       embedding_size = embedding_dim,
                       filter_sizes = list(map(int, filter_sizes.split(","))),
                       num_filters = num_filters,
                       l2_reg_lambda = l2_reg_lambda)
                 # Define Training procedure
                 optimizer = tf.train.AdamOptimizer(1e-3)
                 grads_and_vars = optimizer.compute_gradients(cnn.loss)
                 if is_sync == 1:
                    #同步模式计算更新梯度
                    rep_op = tf.train.SyncReplicasOptimizer(optimizer,
                                                            replicas_to_aggregate=len(
                                                              worker_spec),
                                                            total_num_replicas=len(
                                                              worker_spec),
                                                            use_locking=True)
                    train_op = rep_op.apply_gradients(grads_and_vars,
                                                   global_step=global_step)
                    init_token_op = rep_op.get_init_tokens_op()
                    chief_queue_runner = rep_op.get_chief_queue_runner()
                 else:
                     #Asynchronous mode calculation update gradient
                     train_op = optimizer.apply_gradients(grads_and_vars,
                                                   global_step=global_step)
                 init_op = tf.global_variables_initializer()
                 saver = tf.train.Saver(max_to_keep=num_checkpoints)
                 # Keep track of gradient values and sparsity (optional)
                 grad_summaries = []
                 for g, v in grads_and_vars:
                    if g is not None:
                        grad_hist_summary = tf.summary.histogram("{}/grad/hist".format(v.name), g)
                        sparsity_summary = tf.summary.scalar("{}/grad/sparsity".format(v.name), tf.nn.zero_fraction(g))
                        grad_summaries.append(grad_hist_summary)
                        grad_summaries.append(sparsity_summary)
                 grad_summaries_merged = tf.summary.merge(grad_summaries)
                 # Summaries for loss and accuracy
                 loss_summary = tf.summary.scalar("loss", cnn.loss)
                 acc_summary = tf.summary.scalar("accuracy", cnn.accuracy)
                 # Train Summaries
                 train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])
                 # Dev summaries
                 dev_summary_op = tf.summary.merge([loss_summary, acc_summary])

        sess_config = tf.ConfigProto(
            allow_soft_placement=True,
            log_device_placement=False,
            device_filters =["/job:ps",
                             "/job:worker/task:%d"%task_index]
        )
        # train_dir = tempfile.mkdtemp()
        sv = tf.train.Supervisor(is_chief=(task_index == 0),
                            logdir=log_dir,
                            init_op=init_op,
                            summary_op=None,
                            saver=saver,
                            global_step=global_step

                            )

        with sv.prepare_or_wait_for_session(server.target,config=sess_config) as sess:
            # train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)
            # dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)
            if task_index == 0 and is_sync == 1:
                sv.start_queue_runners(sess, [chief_queue_runner])
                sess.run(init_token_op)
                       # Initialize all variables
            sess.run(init_op)

            def train_step(x_batch, y_batch):
                """
                A single training step
                """
                feed_dict = {
                  cnn.input_x: x_batch,
                  cnn.input_y: y_batch,
                  cnn.dropout_keep_prob: dropout_keep_prob
                }

                if sv.is_chief:
                  sv.summary_computed(sess,sess.run(train_summary_op,feed_dict))
                _, step, summaries, loss, accuracy = sess.run(
                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],
                    feed_dict)
                time_str = datetime.datetime.now().isoformat()
                if step % 50 ==0:
                   print("{}: step {}, loss {:g}, acc {:g}".format(time_str, step, loss, accuracy))

            def dev_step(x_batch, y_batch, writer=None):
                """
                Evaluates model on a dev set
                """
                feed_dict = {
                  cnn.input_x: x_batch,
                  cnn.input_y: y_batch,
                  cnn.dropout_keep_prob: 1.0
                }
                step, summaries, loss, accuracy = sess.run(
                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],
                    feed_dict)
                if sv.is_chief:
                  sv.summary_computed(sess,sess.run(dev_summary_op,feed_dict))
                time_str = datetime.datetime.now().isoformat()
                print("{}: step {}, loss {:g}, acc {:g}".format(time_str, step, loss, accuracy))

            # Generate batches
            batches = batch_iter(
                list(zip(x_train, y_train)), batch_size, num_epochs)

            # Training loop. For each batch...
            for batch in batches:
                x_batch, y_batch = zip(*batch)
                train_step(x_batch, y_batch)
                current_step = tf.train.global_step(sess, global_step)
                if sv.is_chief and current_step % evaluate_every == 0:
                    print("\nEvaluation:")
                    dev_step(x_test,y_test)
                    print("")
                if sv.is_chief and current_step % checkpoint_every == 0:
                    sv.saver.save(sess,checkpoint_prefix,global_step=current_step)
                    print("Saved model checkpoint to {}\n".format(checkpoint_prefix))
        sv.stop()

else:
    x_train, x_test, y_train, y_test =data_preprocess()
    print("Training...")
    # Training
    # =======================================================
    with tf.Graph().as_default():
        session_conf = tf.ConfigProto(
            allow_soft_placement = allow_soft_placement,
            log_device_placement = log_device_placement)

        sess = tf.Session(config = session_conf)
        with sess.as_default():
            cnn = URLCNN(
            sequence_length = x_train.shape[1],
            num_classes = y_train.shape[1],
            embedding_size = embedding_dim,
            filter_sizes = [filter_sizes],
            num_filters = num_filters,
            l2_reg_lambda = l2_reg_lambda)

            # Define Training procedure
            optimizer = tf.train.AdamOptimizer(1e-3)
            grads_and_vars = optimizer.compute_gradients(cnn.loss)
            global_step = tf.Variable(0, name="global_step", trainable=False)
            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)

            # Keep track of gradient values and sparsity (optional)
            grad_summaries = []
            for g, v in grads_and_vars:
                if g is not None:
                    grad_hist_summary = tf.summary.histogram("{}/grad/hist".format(v.name), g)
                    sparsity_summary = tf.summary.scalar("{}/grad/sparsity".format(v.name), tf.nn.zero_fraction(g))
                    grad_summaries.append(grad_hist_summary)
                    grad_summaries.append(sparsity_summary)
            grad_summaries_merged = tf.summary.merge(grad_summaries)

            # Output directory for models and summaries
            print("Writing to {}\n".format(out_dir))

            # Summaries for loss and accuracy
            loss_summary = tf.summary.scalar("loss", cnn.loss)
            acc_summary = tf.summary.scalar("accuracy", cnn.accuracy)

            # Train Summaries
            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])
            train_summary_dir = os.path.join(out_dir, "summaries", "train")
            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)

            # Dev summaries
            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])
            dev_summary_dir = os.path.join(out_dir, "summaries", "dev")
            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)

            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it
            checkpoint_dir = os.path.abspath(os.path.join(out_dir, "checkpoints"))
            checkpoint_prefix = os.path.join(checkpoint_dir, "model")
            if not os.path.exists(checkpoint_dir):
                os.makedirs(checkpoint_dir)
            saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)

            # Initialize all variables
            sess.run(tf.global_variables_initializer())

            def train_step(x_batch, y_batch):
                """
                A single training step
                """
                feed_dict = {
                  cnn.input_x: x_batch,
                  cnn.input_y: y_batch,
                  cnn.dropout_keep_prob: dropout_keep_prob
                }
                _, step, summaries, loss, accuracy = sess.run(
                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],
                    feed_dict)
                time_str = datetime.datetime.now().isoformat()
                if step % 50 ==0:
                   print("{}: step {}, loss {:g}, acc {:g}".format(time_str, step, loss, accuracy))
                train_summary_writer.add_summary(summaries, step)

            def dev_step(x_batch, y_batch, writer=None):
                """
                Evaluates model on a dev set
                """
                feed_dict = {
                  cnn.input_x: x_batch,
                  cnn.input_y: y_batch,
                  cnn.dropout_keep_prob: 1.0
                }
                step, summaries, loss, accuracy = sess.run(
                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],
                    feed_dict)
                time_str = datetime.datetime.now().isoformat()
                print("{}: step {}, loss {:g}, acc {:g}".format(time_str, step, loss, accuracy))
                if writer:
                    writer.add_summary(summaries, step)

            # Generate batches
            batches = batch_iter(
                list(zip(x_train, y_train)), batch_size, num_epochs)

            # Training loop. For each batch...
            for batch in batches:
                x_batch, y_batch = zip(*batch)
                train_step(x_batch, y_batch)
                current_step = tf.train.global_step(sess, global_step)
                if current_step % evaluate_every == 0:
                    print("\nEvaluation:")
                    dev_step(x_test, y_test, writer=dev_summary_writer)
                    print("")
                if current_step % checkpoint_every == 0:
                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)
                    print("Saved model checkpoint to {}\n".format(path))

# Parameters
# ==================================================

# Data Parameters
single_url = None
input_text_file = "data2.csv"

# Eval Parameters
batch_size = 64
checkpoint_dir = "./runs/outputs/checkpoints/"
eval_train = True
# Misc Parameters
allow_soft_placement = True
log_device_placement = False

# validate
# ==================================================

# validate checkout point file
checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)
if checkpoint_file is None:
    print("Cannot find a valid checkpoint file!")
    exit(0)
print("Using checkpoint file : {}".format(checkpoint_file))

# validate word2vec model file
trained_word2vec_model_file = os.path.join(checkpoint_dir, "..", "trained_word2vec.model")
if not os.path.exists(trained_word2vec_model_file):
    print("Word2vec model file \'{}\' doesn't exist!".format(trained_word2vec_model_file))
print("Using word2vec model file : {}".format(trained_word2vec_model_file))

# validate training params file
training_params_file = os.path.join(checkpoint_dir, "..", "training_params.pickle")
if not os.path.exists(training_params_file):
    print("Training params file \'{}\' is missing!".format(training_params_file))
print("Using training params file : {}".format(training_params_file))

# Load params
params = loadDict(training_params_file)
num_labels = int(params['num_labels'])
max_document_length = int(params['max_document_length'])
# Load data
if eval_train and single_url is None:
    x_raw, y_test = load_data_and_labels(input_text_file)
elif single_url is not None:
    x_raw = [single_url]
    y_test=None
else:
    x_raw = ["a masterpiece four years in the making", "everything is off."]
    y_test = [1, 0]

# Get Embedding vector x_test
sentences, max_document_length = padding_sentences(x_raw, '<PADDING>', padding_sentence_length = max_document_length)
x_test = np.array(embedding_sentences(sentences, file_to_load = trained_word2vec_model_file))
print("x_test.shape = {}".format(x_test.shape))


# Evaluation
# ==================================================
print("\nEvaluating...\n")
checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)
graph = tf.Graph()
with graph.as_default():
    session_conf = tf.ConfigProto(
      allow_soft_placement=allow_soft_placement,
      log_device_placement=log_device_placement)
    sess = tf.Session(config=session_conf)
    with sess.as_default():
        # Load the saved meta graph and restore variables
        saver = tf.train.import_meta_graph("{}.meta".format(checkpoint_file))
        saver.restore(sess, checkpoint_file)

        # Get the placeholders from the graph by name
        input_x = graph.get_operation_by_name("input_x").outputs[0]
        # input_y = graph.get_operation_by_name("input_y").outputs[0]
        dropout_keep_prob = graph.get_operation_by_name("dropout_keep_prob").outputs[0]

        # Tensors we want to evaluate
        predictions = graph.get_operation_by_name("output/predictions").outputs[0]

        # Generate batches for one epoch
        batches = batch_iter(list(x_test),batch_size, 1, shuffle=False)

        # Collect the predictions here
        all_predictions = []

        for x_test_batch in batches:
            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})
            all_predictions = np.concatenate([all_predictions, batch_predictions])

# Print accuracy if y_test is defined
if y_test is not None:
    correct_predictions=0
    # correct_predictions = float(sum((all_predictions == y_test)))
    for i in range(0,len(all_predictions)):
       if all_predictions[i]==y_test[i][1]:
           correct_predictions=correct_predictions+1
    correct_predictions=float(correct_predictions)
    print("Total number of test examples: {}".format(len(y_test)))
    print("Accuracy: {:g}".format(correct_predictions/float(len(y_test))))

if single_url is not None:
    print(sentences)
    print("Result:", all_predictions)
else:
    # Save the evaluation to a csv
    predictions_human_readable = np.column_stack((np.array([text for text in x_raw]), all_predictions))
    out_path = os.path.join(checkpoint_dir, "..", "prediction.csv")
    print("Saving evaluation to {0}".format(out_path))
    with open(out_path, 'w') as f:
        csv.writer(f).writerows(predictions_human_readable)

